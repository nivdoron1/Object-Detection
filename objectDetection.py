# -*- coding: utf-8 -*-
"""objectdetection (2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1znXXkygWOGaXAxZYkdXBVhV7Bo9WYrGL
"""

!pip install matplotlib
!pip install roboflow
from roboflow import Roboflow
import torch
import torchvision
from torch.utils.data import Dataset
from torchvision.models.detection import fasterrcnn_mobilenet_v3_large_320_fpn, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights
from torchvision.models.detection.backbone_utils import BackboneWithFPN
from torchvision.models.detection.faster_rcnn import FasterRCNN, FastRCNNPredictor
from torchvision.models.detection.rpn import AnchorGenerator
from torchvision.models.mobilenet import mobilenet_v2
from torchvision.transforms import Compose, ToTensor, RandomHorizontalFlip
from torch.utils.data import DataLoader, RandomSampler
from torchvision.datasets import VOCDetection
from torch.optim import SGD
from torch.optim.lr_scheduler import StepLR
import numpy as np
import os
from PIL import Image
import matplotlib.pyplot as plt
from torchvision.ops import box_iou
from torchvision.transforms import functional as F
import torchvision.transforms as T
from torch.utils.data import Dataset
import os
import torch
import json
from PIL import Image
from lxml import etree
import matplotlib.pyplot as plt

from roboflow import Roboflow
rf = Roboflow(api_key="mtilPA6tDtVzgj3nVogW")
project = rf.workspace("jacob-solawetz").project("pascal-voc-2012")
dataset = project.version(1).download("voc")

from google.colab import drive
drive.mount('/content/drive')

from torch.utils.data import Dataset
import os
import xml.etree.ElementTree as ET
from PIL import Image
import torch
from torchvision import transforms

class PascalVOCDataset(Dataset):
    def __init__(self, root_dir, split="train", transform=None):
        """
        Args:
            root_dir (string): Directory with all the dataset parts.
            split (string): One of {"train", "valid"} indicating which dataset to load.
            transform (callable, optional): Optional transform to be applied on a sample.
        """
        self.root_dir = os.path.join(root_dir, split)
        self.transform = transform
        # Filter out the files; assume XML annotations have the same name as images
        self.image_files = [file for file in os.listdir(self.root_dir) if file.endswith('.jpg')]

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        img_file = self.image_files[idx]
        base_name = self.image_files[idx]
        if base_name.lower().endswith(".jpg"):
            base_name = base_name.rsplit(".jpg", 1)[0] + ".xml"
        elif base_name.lower().endswith(".jpeg"):
            base_name = base_name.rsplit(".jpeg", 1)[0] + ".xml"
        img_path = os.path.join(self.root_dir, img_file)
        annotation_path = os.path.join(self.root_dir,base_name)
        image = Image.open(img_path).convert("RGB")

        # Parse XML annotation
        boxes = []
        labels = []
        area = []
        iscrowd = [] # Assuming all instances are single objects
        tree = ET.parse(annotation_path)
        root = tree.getroot()
        for member in root.findall('object'):
            labels.append(member.find('name').text)
            bndbox = member.find('bndbox')
            xmin = int(bndbox.find('xmin').text)
            ymin = int(bndbox.find('ymin').text)
            xmax = int(bndbox.find('xmax').text)
            ymax = int(bndbox.find('ymax').text)
            boxes.append([xmin, ymin, xmax, ymax])
            area.append((xmax - xmin) * (ymax - ymin))
            iscrowd.append(0)

        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        labels = torch.as_tensor([self.class_to_idx(label) for label in labels], dtype=torch.int64)
        area = torch.as_tensor(area, dtype=torch.float32)
        iscrowd = torch.as_tensor(iscrowd, dtype=torch.int64)

        target = {}
        target["boxes"] = boxes
        target["labels"] = labels
        target["image_id"] = torch.tensor([idx])
        target["area"] = area
        target["iscrowd"] = iscrowd

        if self.transform:
            image = self.transform(image)

        return image, target

    # Make sure this dictionary matches the class names in your annotation files
    def class_to_idx(self, class_name):
        class_dict = {
            'aeroplane': 1, 'bicycle': 2, 'bird': 3, 'boat': 4, 'bottle': 5,
            'bus': 6, 'car': 7, 'cat': 8, 'chair': 9, 'cow': 10,
            'diningtable': 11, 'dog': 12, 'horse': 13, 'motorbike': 14, 'person': 15,
            'pottedplant': 16, 'sheep': 17, 'sofa': 18, 'train': 19, 'tvmonitor': 20
        }
        return class_dict.get(class_name, 0)

# Define your transformations
transform = {
    "train": transforms.Compose([transforms.ToTensor(),
                                  transforms.RandomHorizontalFlip(0.5)]),
    "val": transforms.Compose([transforms.ToTensor()])
}

def collate_fn(batch):
    return tuple(zip(*batch))

classes = ["person", "chair", "car", "dog", "bottle", "cat", "bird", "pottedplant",
           "sheep", "boat", "aeroplane", "tvmonitor", "sofa", "bicycle", "horse",
           "diningtable", "motorbike", "cow", "train", "bus"]

# Define hyperparameters
input_size = (32, 32, 3)
learning_rate = 1e-3
batch_size = 8
aspect_ratio_group_factor = 3
amp = False
num_classes = len(classes)+1
momentum=0.9
weight_decay=0.0005

import bisect
import copy
import math
from collections import defaultdict
from itertools import chain, repeat

import numpy as np
import torch
import torch.utils.data
import torchvision
from PIL import Image
from torch.utils.data.sampler import BatchSampler, Sampler
from torch.utils.model_zoo import tqdm


def _repeat_to_at_least(iterable, n):
    repeat_times = math.ceil(n / len(iterable))
    repeated = chain.from_iterable(repeat(iterable, repeat_times))
    return list(repeated)


class GroupedBatchSampler(BatchSampler):
    def __init__(self, sampler, group_ids, batch_size):
        if not isinstance(sampler, Sampler):
            raise ValueError(f"sampler should be an instance of torch.utils.data.Sampler, but got sampler={sampler}")
        self.sampler = sampler
        self.group_ids = group_ids
        self.batch_size = batch_size

    def __iter__(self):
        buffer_per_group = defaultdict(list)
        samples_per_group = defaultdict(list)

        num_batches = 0
        for idx in self.sampler:
            group_id = self.group_ids[idx]
            buffer_per_group[group_id].append(idx)
            samples_per_group[group_id].append(idx)
            if len(buffer_per_group[group_id]) == self.batch_size:
                yield buffer_per_group[group_id]
                num_batches += 1
                del buffer_per_group[group_id]
            assert len(buffer_per_group[group_id]) < self.batch_size

        expected_num_batches = len(self)
        num_remaining = expected_num_batches - num_batches
        if num_remaining > 0:
            for group_id, _ in sorted(buffer_per_group.items(), key=lambda x: len(x[1]), reverse=True):
                remaining = self.batch_size - len(buffer_per_group[group_id])
                samples_from_group_id = _repeat_to_at_least(samples_per_group[group_id], remaining)
                buffer_per_group[group_id].extend(samples_from_group_id[:remaining])
                assert len(buffer_per_group[group_id]) == self.batch_size
                yield buffer_per_group[group_id]
                num_remaining -= 1
                if num_remaining == 0:
                    break
        assert num_remaining == 0

    def __len__(self):
        return len(self.sampler) // self.batch_size


def _compute_aspect_ratios_slow(dataset, indices=None):
    if indices is None:
        indices = range(len(dataset))

    class SubsetSampler(Sampler):
        def __init__(self, indices):
            self.indices = indices

        def __iter__(self):
            return iter(self.indices)

        def __len__(self):
            return len(self.indices)

    sampler = SubsetSampler(indices)
    data_loader = torch.utils.data.DataLoader(
        dataset,
        batch_size=1,
        sampler=sampler,
        num_workers=14,  # you might want to increase it for faster processing
        collate_fn=lambda x: x[0],
    )
    aspect_ratios = []
    with tqdm(total=len(dataset)) as pbar:
        for _i, (img, _) in enumerate(data_loader):
            pbar.update(1)
            height, width = img.shape[-2:]
            aspect_ratio = float(width) / float(height)
            aspect_ratios.append(aspect_ratio)
    return aspect_ratios


def _compute_aspect_ratios_custom_dataset(dataset, indices=None):
    if indices is None:
        indices = range(len(dataset))
    aspect_ratios = []
    for i in indices:
        height, width = dataset.get_height_and_width(i)
        aspect_ratio = float(width) / float(height)
        aspect_ratios.append(aspect_ratio)
    return aspect_ratios


def _compute_aspect_ratios_coco_dataset(dataset, indices=None):
    if indices is None:
        indices = range(len(dataset))
    aspect_ratios = []
    for i in indices:
        img_info = dataset.coco.imgs[dataset.ids[i]]
        aspect_ratio = float(img_info["width"]) / float(img_info["height"])
        aspect_ratios.append(aspect_ratio)
    return aspect_ratios


def _compute_aspect_ratios_voc_dataset(dataset, indices=None):
    if indices is None:
        indices = range(len(dataset))
    aspect_ratios = []
    for i in indices:
        # this doesn't load the data into memory, because PIL loads it lazily
        width, height = Image.open(dataset.images[i]).size
        aspect_ratio = float(width) / float(height)
        aspect_ratios.append(aspect_ratio)
    return aspect_ratios


def _compute_aspect_ratios_subset_dataset(dataset, indices=None):
    if indices is None:
        indices = range(len(dataset))

    ds_indices = [dataset.indices[i] for i in indices]
    return compute_aspect_ratios(dataset.dataset, ds_indices)


def compute_aspect_ratios(dataset, indices=None):
    if hasattr(dataset, "get_height_and_width"):
        return _compute_aspect_ratios_custom_dataset(dataset, indices)

    if isinstance(dataset, torchvision.datasets.CocoDetection):
        return _compute_aspect_ratios_coco_dataset(dataset, indices)

    if isinstance(dataset, torchvision.datasets.VOCDetection):
        return _compute_aspect_ratios_voc_dataset(dataset, indices)

    if isinstance(dataset, torch.utils.data.Subset):
        return _compute_aspect_ratios_subset_dataset(dataset, indices)

    # slow path
    return _compute_aspect_ratios_slow(dataset, indices)


def _quantize(x, bins):
    bins = copy.deepcopy(bins)
    bins = sorted(bins)
    quantized = list(map(lambda y: bisect.bisect_right(bins, y), x))
    return quantized


def create_aspect_ratio_groups(dataset, k=0):
    aspect_ratios = compute_aspect_ratios(dataset)
    bins = (2 ** np.linspace(-1, 1, 2 * k + 1)).tolist() if k > 0 else [1.0]
    groups = _quantize(aspect_ratios, bins)
    # count number of elements per group
    counts = np.unique(groups, return_counts=True)[1]
    fbins = [0] + bins + [np.inf]
    print(f"Using {fbins} as bins for aspect ratio quantization")
    print(f"Count of instances per bin: {counts}")
    return groups

import matplotlib.patches as patches
import matplotlib.pyplot as plt
import os
# Initialize your dataset
train_dataset = PascalVOCDataset(root_dir='/content/Pascal-VOC-2012-1', split='train', transform=transform["train"])
valid_dataset = PascalVOCDataset(root_dir='/content/Pascal-VOC-2012-1', split='valid', transform=transform["val"])

# Function to visualize a single image with bounding boxes
def show_image_with_boxes(image, target, ax=None):
    """Show image with bounding boxes."""
    if ax is None:
        _, ax = plt.subplots(1, 1, figsize=(12, 9))
    ax.imshow(image.permute(1, 2, 0))  # Rearrange channels to match image format for display

    for box in target["boxes"]:
        rect = patches.Rectangle(
            (box[0], box[1]), box[2] - box[0], box[3] - box[1],
            linewidth=1, edgecolor='r', facecolor='none')
        ax.add_patch(rect)

    plt.axis('off')
    plt.show()

# Helper function to display the first 5 images in a dataset
def show_first_five(dataset):
    for i in range(5):
        image, target = dataset[i]
        show_image_with_boxes(image, target)

nw = min([os.cpu_count(), batch_size if batch_size > 1 else 0, 4])  # number of workers
print('Using {} dataloader workers every process'.format(nw))
if aspect_ratio_group_factor >= 0:
    train_sampler = torch.utils.data.RandomSampler(train_dataset)
    group_ids = create_aspect_ratio_groups(train_dataset, k=aspect_ratio_group_factor)
    train_batch_sampler = GroupedBatchSampler(train_sampler, group_ids, batch_size)

nw = min([os.cpu_count(), batch_size if batch_size > 1 else 0, 4])  # number of workers
print('Using %g dataloader workers' % nw)

if train_sampler:
    train_dataloader = torch.utils.data.DataLoader(train_dataset,
                                                    batch_sampler=train_batch_sampler,
                                                    pin_memory=True,
                                                    num_workers=nw,
                                                    collate_fn=collate_fn)
else:
    train_dataloader = torch.utils.data.DataLoader(train_dataset,
                                                    batch_size=batch_size,
                                                    shuffle=True,
                                                    pin_memory=True,
                                                    num_workers=nw,
                                                    collate_fn=collate_fn)

val_dataloader = torch.utils.data.DataLoader(valid_dataset,
                                              batch_size=1,
                                              shuffle=False,
                                              pin_memory=True,
                                              num_workers=nw,
                                              collate_fn=collate_fn)
print("Training Set:")
show_first_five(train_dataset)

print("Validation Set:")
show_first_five(valid_dataset)

# Report split sizes
print('Training set has {} instances'.format(len(train_dataset)))
print('Validation set has {} instances'.format(len(valid_dataset)))

def visualize_sample(dataset, idx):
    image, target = dataset[idx]
    img = transforms.ToPILImage()(image)
    plt.imshow(img)
    print(f"Labels: {target['labels']}")
    plt.title(f"Labels: {target['labels']}")
    plt.show()

for i in range(5):
    visualize_sample(train_dataset, i)

from torchvision.models.detection import fasterrcnn_mobilenet_v3_large_320_fpn
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from torchvision.models.detection import FasterRCNN_MobileNet_V3_Large_320_FPN_Weights

def get_model(num_classes):
    # Load a pre-trained model for fine-tuning with the most up-to-date weights
    weights = FasterRCNN_MobileNet_V3_Large_320_FPN_Weights.DEFAULT
    model = fasterrcnn_mobilenet_v3_large_320_fpn(weights=weights)

    in_features = model.roi_heads.box_predictor.cls_score.in_features
    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes )

    return model

# Get the model with specified weights
model = get_model(num_classes=21)

# Move model to the device (GPU/CPU)
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model.to(device)
print("Model successfully created!")

if not os.path.exists("save_weights"):
  os.makedirs("save_weights")

scaler = torch.cuda.amp.GradScaler() if amp else None
train_loss = []
learning_rate = []
val_map = []

loss_fn = torch.nn.CrossEntropyLoss()
for param in model.backbone.parameters():
    param.requires_grad = False
params = [p for p in model.parameters() if p.requires_grad]
optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer,
                                                   step_size=3,
                                                   gamma=0.1)

# Function to save model checkpoints with accuracy and loss
def save_checkpoint(model, optimizer, epoch, train_loss, accuracy, path="checkpoint.pth"):
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'train_loss': train_loss,  # Save train loss
        'accuracy': accuracy  # Save valid_loss
    },
    path)

def train_one_epoch(epoch_index,train_dataloader, tb_writer):
    running_loss = 0.0
    running_classifier_loss = 0.0
    running_box_reg_loss = 0.0
    running_objectness_loss = 0.0
    running_rpn_box_reg_loss = 0.0
    count = 0
    for i, (images, targets) in enumerate(train_dataloader):
        valid_samples = [j for j, t in enumerate(targets) if t['boxes'].shape[0] > 0]
        images = [images[j].to(device) for j in valid_samples]
        targets = [{k: v.to(device) for k, v in targets[j].items()} for j in valid_samples]
        # Zero your gradients for every batch!
        optimizer.zero_grad()

        # Make predictions for this batch and calculate the losses
        loss_dict = model(images, targets)

        # Sum up all losses
        losses = sum(loss for loss in loss_dict.values())

        # Compute the backward gradients and adjust learning weights
        losses.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)
        optimizer.step()

        # Update running loss
        running_loss += losses.item()
        running_classifier_loss += loss_dict['loss_classifier'].item()
        running_box_reg_loss += loss_dict['loss_box_reg'].item()
        running_objectness_loss += loss_dict['loss_objectness'].item()
        running_rpn_box_reg_loss += loss_dict['loss_rpn_box_reg'].item()

        if i % 50 == 49:
            # Calculate the average loss over the last 50 batches
            last_loss = running_loss / 50
            last_classifier_loss = running_classifier_loss / 50
            last_box_reg_loss = running_box_reg_loss / 50
            last_objectness_loss = running_objectness_loss / 50
            last_rpn_box_reg_loss = running_rpn_box_reg_loss / 50

            print(f"Batch {i + 1} Average Losses: Total: {last_loss}, Classifier: {last_classifier_loss}, Box Reg: {last_box_reg_loss}, Objectness: {last_objectness_loss}, RPN Box Reg: {last_rpn_box_reg_loss}")

            tb_x = epoch_index * len(train_dataloader) + i + 1
            tb_writer.add_scalar('Loss/train', last_loss, tb_x)
            tb_writer.add_scalar('Loss/classifier', last_classifier_loss, tb_x)
            tb_writer.add_scalar('Loss/box_reg', last_box_reg_loss, tb_x)
            tb_writer.add_scalar('Loss/objectness', last_objectness_loss, tb_x)
            tb_writer.add_scalar('Loss/rpn_box_reg', last_rpn_box_reg_loss, tb_x)

            # Reset the running loss for the next 1000 batches
            running_loss = 0.0
            running_classifier_loss = 0.0
            running_box_reg_loss = 0.0
            running_objectness_loss = 0.0
            running_rpn_box_reg_loss = 0.0

    # Return the average loss over the epoch
    avg_loss = losses.item()
    return avg_loss

def calculate_iou(box1, box2):
    """Calculate the Intersection over Union (IoU) of two bounding boxes."""
    # Determine the coordinates of the intersection rectangle
    x_left = max(box1[0], box2[0])
    y_top = max(box1[1], box2[1])
    x_right = min(box1[2], box2[2])
    y_bottom = min(box1[3], box2[3])

    # Calculate area of intersection rectangle
    intersection_area = (x_right - x_left) * (y_bottom - y_top)

    # Calculate area of both bounding boxes
    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])
    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])

    # Calculate union area by using the formula: union(A,B) = A + B - intersection(A,B)
    union_area = box1_area + box2_area - intersection_area

    # Compute the IoU
    iou = intersection_area / union_area
    return iou


def calculate_object_detection_accuracy(outputs, targets, iou_threshold=0.5):
    correct = 0
    total = 0
    for output, target in zip(outputs, targets):
        pred_boxes = output['boxes'].cpu().detach()
        true_boxes = target['boxes'].cpu().detach()

        if len(true_boxes) == 0 or len(pred_boxes) == 0:

            # Skip this image since there are no ground truth boxes or predictions
            continue

        for pred_box in pred_boxes:
            ious = [calculate_iou(pred_box, true_box) for true_box in true_boxes]
            max_iou = max(ious) if ious else 0

            if max_iou >= iou_threshold:
                correct += 1

        total += len(true_boxes)

    if total == 0:
        return 0
    else:
        return correct / total

def validate(data_loader, device):
    val_loss = 0
    accuracy = 0
    correct = 0
    total = 0
    accuracy_list = []
    with torch.no_grad():  # No need to track gradients
        for images, targets in data_loader:
            valid_samples = [j for j, t in enumerate(targets) if t['boxes'].shape[0] > 0]
            if not valid_samples:  # Check if there are no valid samples
                continue  # Skip this batch

            images = [images[j].to(device) for j in valid_samples ]
            targets = [{k: v.to(device) for k, v in targets[j].items()} for j in valid_samples]
            outputs = model(images)
            # Calculate accuracy (or any other metrics)
            acc = calculate_object_detection_accuracy(outputs, targets)
            accuracy_list.append(acc)

    accuracy = sum(accuracy_list) / len(accuracy_list) if accuracy_list else 0
    return accuracy

from datetime import datetime
from torch.utils.tensorboard import SummaryWriter
import torch.nn as nn

# Initializing in a separate cell so we can easily add more epochs to the same run
timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))
criterion = nn.CrossEntropyLoss()
epoch_number = 0
loss_list = []
accuracy_list=[]
vloss_list=[]
EPOCHS = 5

for epoch in range(EPOCHS):
    print('EPOCH {}:'.format(epoch_number + 1))

    # Make sure gradient tracking is on, and do a pass over the data
    model.train(True)
    avg_loss = train_one_epoch(epoch_number,train_dataloader, writer)
    loss_list.append(avg_loss)
    print(f'avg_loss:{avg_loss}')

    running_vloss = 0.0
    model.train(False)
    # Make sure gradient tracking is off, and do a pass over the data
    model.eval()
    accuracy = validate(val_dataloader, device)
    print('Accuracy: {}'.format(accuracy))
    accuracy_list.append(accuracy)

    #avg_vloss = total_vloss / total_samples if total_samples > 0 else 0.0
    #vloss_list.append(avg_vloss)
    writer.add_scalar('Loss/Training', avg_loss, epoch_number + 1)
    writer.add_scalar('Accuracy/Validation', accuracy, epoch_number + 1)
    writer.flush()
    epoch_number += 1
    # Save the checkpoint for the current epoch
    checkpoint_path = f"./drive/MyDrive/ModelCheckpointV4/checkpoint_epoch_{epoch}.pth"
    save_checkpoint(model, optimizer, epoch, train_loss=avg_loss, accuracy=accuracy, path=checkpoint_path)
    scheduler.step()
writer.close()

import matplotlib.pyplot as plt

def plot_loss(train_losses,title,label):
    """
    Plot the training and validation loss over epochs.

    :param train_losses: A list of training loss values, one for each epoch.
    """
    epochs = range(1, len(train_losses) + 1)
    plt.plot(epochs, train_losses, '-o', label=label)
    plt.xlabel('Epochs')
    plt.ylabel(label)
    plt.title(title)
    plt.legend()
    plt.grid(True)
    plt.show()

init_epochs=5
for name, parameter in model.backbone.named_parameters():
    split_name = name.split(".")[0]
    if split_name in ["0", "1", "2", "3"]:
        parameter.requires_grad = False
    else:
        parameter.requires_grad = True
    print(f"{name} requires_grad: {parameter.requires_grad}")
# define optimizer
params = [p for p in model.parameters() if p.requires_grad]
optimizer = torch.optim.SGD(params, lr=0.005,
                            momentum=0.9, weight_decay=0.0005)
# learning rate scheduler
lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,
                                                step_size=3,
                                                gamma=0.33)
num_epochs = 10
for epoch in range(init_epochs, num_epochs+init_epochs, 1):
    model.train(True)
    # train for one epoch, printing every 50 iterations
    avg_loss = train_one_epoch(epoch,train_dataloader, writer)
    loss_list.append(avg_loss)
    model.eval()

    #accuracy = eval_network(model,val_dataloader,device)
    #print('Accuracy: {}'.format(accuracy))
    #accuracy_list.append(accuracy)
    accuracy = validate(val_dataloader, device)
    print('Accuracy: {}'.format(accuracy))
    accuracy_list.append(accuracy)

    #avg_vloss = total_vloss / total_samples if total_samples > 0 else 0.0
    #vloss_list.append(avg_vloss)
    writer.add_scalar('Loss/Training', avg_loss, epoch_number + 1)
    writer.flush()
    epoch_number += 1
    # Save the checkpoint for the current epoch
    checkpoint_path = f"./drive/MyDrive/ModelCheckpointV5/checkpoint_epoch_{epoch}.pth"
    save_checkpoint(model, optimizer, epoch, train_loss=avg_loss, accuracy=accuracy, path=checkpoint_path)
    # update the learning rate
    lr_scheduler.step()

print(len(loss_list))
plot_loss(loss_list,"Training Loss Transformation Over Epochs","Training loss")
print(len(accuracy_list))
plot_loss(accuracy_list,"Accuracy Transformation Over Epochs","Accuracy")

def predict(model, img_path, device, classes, threshold=0.5):
    model.eval()  # Ensure the model is in eval mode
    img = Image.open(img_path).convert("RGB")

    # Use the correct transform
    # Assuming 'transform' is a dictionary with preprocessing steps
    img_transformed = transform["val"](img)

    # Add a batch dimension and send the image to the device
    img_tensor = img_transformed.unsqueeze(0).to(device)

    with torch.no_grad():
        prediction = model(img_tensor)

    # Process prediction results
    classified_objects = []
    labels = prediction[0]['labels'].cpu().numpy()
    boxes = prediction[0]['boxes'].cpu().numpy()
    scores = prediction[0]['scores'].cpu().numpy()

    for label, box, score in zip(labels, boxes, scores):
        if score > threshold:
            class_name = classes[label]
            classified_objects.append({"class": class_name, "box": box.tolist(), "score": score.item()})

    return classified_objects

import cv2
import torch
from torchvision.transforms import functional as F

def save_video_with_predicted_bboxes(video_path, output_video_path, model, device, classes, transform, threshold=0.5):
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print("Error: Could not open video.")
        return

    # Read the first frame to get the video dimensions
    ret, first_frame = cap.read()
    if not ret:
        print("Error: Could not read the first frame.")
        return

    # Reset the video capture to the first frame
    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)

    # Get video properties
    frame_width = int(cap.get(3))
    frame_height = int(cap.get(4))
    fps = cap.get(cv2.CAP_PROP_FPS)

    # Define the codec and create VideoWriter object
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # For an mp4 output
    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))

    model.eval()
    frame_count = 0  # Initialize frame counter

    with torch.no_grad():
        while True:  # Process until the end of the video
            ret, frame = cap.read()
            if not ret:
                break

            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frame_transformed = transform(F.to_pil_image(frame_rgb)).unsqueeze(0).to(device)
            output = model(frame_transformed)[0]

            # Calculate the timestamp of the current frame in the video
            timestamp = frame_count / fps

            detections = []  # Store detections for the current frame

            for box, label, score in zip(output['boxes'], output['labels'], output['scores']):
                if score >= threshold:
                    box = box.to('cpu').numpy().astype(int)
                    class_name = classes[label]
                    label_text = f"{class_name}"
                    detections.append((class_name, score.item(), box))

                    # Draw bounding box and label on the frame
                    cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)
                    cv2.putText(frame, label_text, (box[0], box[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)
            # Write frame to output video
            out.write(frame)
            frame_count += 1

    # Release everything when job is finished
    cap.release()
    out.release()



video_path = "./drive/MyDrive/images/london2.mp4"
output_video_path = "./drive/MyDrive/images/londonboxesnew.mp4"
epoch = 12
saved_model_path = f"./drive/MyDrive/ModelCheckpointV5/checkpoint_epoch_{epoch}.pth"
checkpoint = torch.load(saved_model_path)
model.load_state_dict(checkpoint['model_state_dict'])
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
classes = ['background','aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person','pottedplanet', 'sheep', 'sofa', 'train', 'tvmonitor']
transforms = transform["train"]  # Adjust as needed

save_video_with_predicted_bboxes(video_path, output_video_path, model, device, classes, transforms)